{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19 continued: \n",
    "# We need to first go back one folder so we will use the same code as is present in the data_ingestion.ipynb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RadhikaMaheshwari\\\\Desktop\\\\Test\\\\DeepLearning\\\\ETE\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RadhikaMaheshwari\\\\Desktop\\\\Test\\\\DeepLearning\\\\ETE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 19 completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 20: We first need to update config.yaml file. \n",
    "# Here we will need to mention data validation related configuration\n",
    "# In this, what we are doing is - we are creating a key called data validation. Inside this we are giving root directory for\n",
    "# storing contents of the data validation work. so we are creating data_validation folder in the artifacts folder. This\n",
    "# path is now defined. \n",
    "# Next we are defining the unzipped data path of the file that was imported during the data ingestion phase.\n",
    "# then we will create a status file so we are defining the path to the status.txt file. \n",
    "# When our data passes validation then we will put True in the status.txt file otherwise False. If it is True then only we will continue\n",
    "# with the next stage of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the above is done then Step 20 is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21: We need to define entity\n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    unzip_data_dir: Path \n",
    "    all_schema: dict \n",
    "\n",
    "# all_schema is that we will be reading the schema file here so we are giving that as a variabnle too. As schema file is a .yaml \n",
    "# file hence we are assigning the path as dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21 Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 22: Defining Configuration manager\n",
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            STATUS_FILE = config.STATUS_FILE,\n",
    "            unzip_data_dir = config.unzip_data_dir,\n",
    "            all_schema=schema\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21 Completed. In the above step we have only grabbed the details which will be required during data validation phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 22: Now we need to create data validation component\n",
    "import os \n",
    "from mlProject import logger \n",
    "import pandas as pd\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self,config: DataValidationConfig):\n",
    "        self.config = config \n",
    "    \n",
    "    def validate_all_columns(self) -> bool: \n",
    "        try:\n",
    "            validation_status = None \n",
    "\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "            all_cols = list(data.columns)\n",
    "\n",
    "            all_schema = self.config.all_schema.keys()\n",
    "\n",
    "            for col in all_cols:\n",
    "                if col not in all_schema:\n",
    "                    validation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status} \\n\")\n",
    "                else:\n",
    "                    validation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status} \\n\")\n",
    "            \n",
    "            return validation_status\n",
    "        except Exception as e:\n",
    "            raise e \n",
    "    \n",
    "    def validate_all_data_type(self) -> bool:\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "            all_cols = list(data.columns)\n",
    "\n",
    "            all_schema = self.config.all_schema\n",
    "\n",
    "            for col in all_cols:\n",
    "                for sch in all_schema:\n",
    "                    # if (col == sch):\n",
    "                    #     print(f\"{all_schema[sch]}\")\n",
    "                    # else:\n",
    "                    #     pass\n",
    "                    if ((col == sch ) and (data.dtypes[col] == all_schema[sch])):\n",
    "                        validation_status = True\n",
    "                        with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                            f.write(f'Validation status from dtypes function: {validation_status} \\n')\n",
    "            \n",
    "            return validation_status\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 22: component preparation completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 22:52:10,557: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-22 22:52:10,568: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-22 22:52:10,583: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-05-22 22:52:10,585: INFO: common: Created Directory at: artifacts]\n",
      "[2024-05-22 22:52:10,586: INFO: common: Created Directory at: artifacts/data_validation]\n"
     ]
    }
   ],
   "source": [
    "# Step 23: Now we need to write the code for pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_config()\n",
    "    data_validation = DataValidation(data_validation_config)\n",
    "    data_validation.validate_all_columns()\n",
    "    data_validation.validate_all_data_type()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 23 Completed\n",
    "#Now we just need to convert this notebook into modular components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 24: Data Transformation\n",
    "# Here we are just going to perform train test split. \n",
    "# Here we can also do PCA, EDA, Feature Engineering. \n",
    "# Our dataset is already clean so we are not performing train test split. \n",
    "# In research folder we will create a file called 03_data_transformation.ipynb\n",
    "#Step 24: completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
